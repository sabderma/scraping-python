from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import csv, time, re

# ---------------- Config navigateur ----------------
options = Options()
options.add_argument("--detach")
driver = webdriver.Chrome(options=options)

BASE = "https://immobilier.lefigaro.fr/annonces/immobilier-vente-bien-paris.html"

driver.get(BASE)

#---alerte --- ici il faut que taccept le boutton cookies du site --------------------------------------------
# ---------------- CSV ----------------
with open("annonces_lefigaro_paris.csv", "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["type", "prix", "prix_m2", "surface", "nb_pieces", "localisation", "details"])

    MAX_PAGES = 5   # ⬅️ augmente si tu veux
    for page in range(1, MAX_PAGES + 1):
        url = BASE if page == 1 else f"{BASE}?page={page}"
        driver.get(url)
        print(f"\n========== PAGE {page} ==========\n")

        # attendre les cartes
        WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.list-annonce li article.classified-card"))
        )
        cards = driver.find_elements(By.CSS_SELECTOR, "ul.list-annonce li article.classified-card")
        print(f"{len(cards)} annonces trouvées.")

        for i, card in enumerate(cards, 1):
            type_bien   = "non disponible"
            prix        = "non disponible"
            prix_m2     = "non disponible"
            surface     = "non disponible"
            nb_pieces   = "non disponible"
            localisation= "non disponible"
            details     = "non disponible"

            # ---- PRIX ----
            try:
                prix = card.find_element(By.CSS_SELECTOR, "div.main-price-wrapper span.main-price").text.strip()
            except NoSuchElementException:
                pass

            # ---- PRIX / m² ----
            try:
                prix_m2 = card.find_element(By.CSS_SELECTOR, "div.price-per-m2-partner span.price-per-m2").text.strip()
            except NoSuchElementException:
                pass

            # ---- TYPE ----
            try:
                type_bien = card.find_element(By.CSS_SELECTOR, "p.classified-card-infos-estate-type").text.strip()
            except NoSuchElementException:
                pass

            # ---- CLÉS (pièces / surface) ----
            try:
                # surface et pièces apparaissent dans les <li> "classified-card-infos-key-items"
                for li in card.find_elements(By.CSS_SELECTOR, "li.classified-card-infos-key-items"):
                    t = li.text.strip()
                    if "m²" in t and surface == "non disponible":
                        surface = re.search(r"(\d+(?:[.,]\d+)?)\s*m²", t).group(0)
                    if "pièce" in t and nb_pieces == "non disponible":
                        m = re.search(r"(\d+)\s*pièce", t, re.IGNORECASE)
                        if m: nb_pieces = f"{m.group(1)} pièces"
            except Exception:
                pass

            # ---- LOCALISATION ----
            try:
                localisation = card.find_element(By.CSS_SELECTOR, "span.classified-card-infos-location").text.strip()
            except NoSuchElementException:
               pass

            # ---- DETAILS courts ----
            details_full = " ".join(card.text.split())
            details = details_full[:160] + ("…" if len(details_full) > 160 else "")

            print(f"{i}. {type_bien} | {prix} | {prix_m2} | {surface} | {nb_pieces} | {localisation}")
            w.writerow([type_bien, prix, prix_m2, surface, nb_pieces, localisation, details])

        time.sleep(1)  # petit délai entre pages

driver.quit()
print("\n✅ Scraping terminé — fichier: annonces_lefigaro_paris.csv")
